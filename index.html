<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jiayun (Peter) Wang </title> <meta name="author" content="Jiayun (Peter) Wang"> <meta name="description" content="Peter Wang's Homepage "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A9&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://peterjwang.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%70%65%74%65%72%77@%63%61%6C%74%65%63%68.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=IBn7PdYAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/samaonline" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/peterjwang" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://twitter.com/Peter_j_Wang" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">Misc </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jiayun</span> (Peter) Wang </h1> <p class="desc">Postdoc, <a href="https://www.cms.caltech.edu/" rel="external nofollow noopener" target="_blank">California Institute of Technology</a> | peterw at caltech dot edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof-480.webp 480w,/assets/img/prof-800.webp 800w,/assets/img/prof-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof.jpg?0831bd8b60d34dcd369a6bdb7953671a" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I am a postdoctoral researcher in the <a href="https://cms.caltech.edu/" rel="external nofollow noopener" target="_blank">Computing + Mathematical Sciences Department</a> at California Institute of Technology, working with <a href="http://tensorlab.cms.caltech.edu/users/anima/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a> . Prior to joining Caltech, I completed my Ph.D. in <a href="https://vision.berkeley.edu/" rel="external nofollow noopener" target="_blank">Vision Science</a> and <a href="https://bair.berkeley.edu/" rel="external nofollow noopener" target="_blank">Berkeley AI Research (BAIR)</a> at <a href="https://www.berkeley.edu/" rel="external nofollow noopener" target="_blank">UC Berkeley</a>, where I worked with <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> on fundamental machine learning and computer vision models and <a href="https://optometry.berkeley.edu/people/meng-lin/" rel="external nofollow noopener" target="_blank">Meng Lin</a> on applying them to healthcare.</p> <p>I am on the 2024-2025 job market looking for full-time positions. Please feel free to reach out!</p> <p><strong>Research Interest:</strong> My research lies at the intersection of machine learning, computer vision and AI for healthcare. My research highlights:</p> <ol> <li> <strong>Minimally Supervised ML.</strong> Self-supervised learning from unlabled data for recognition &amp; detection (<a href="https://pwang.pw/spn.html" rel="external nofollow noopener" target="_blank">TPAMI‚Äô21</a>) and for geometry (<a href="https://pwang.pw/trajSSL" rel="external nofollow noopener" target="_blank">ECCV‚Äô24</a>).</li> <li> <strong>Efficient AI Algorithms.</strong> AI models as duality to the data. Models can be made efficient if they are aware of data structures, such as orthogonality (<a href="https://pwang.pw/ocnn.html" rel="external nofollow noopener" target="_blank">CVPR‚Äô20</a>) and recurrence (<a href="https://pwang.pw/rpg.html" rel="external nofollow noopener" target="_blank">WACV‚Äô23</a>).</li> <li> <strong>AI for Health.</strong> Minimally supervised ML for enhanced clinical effectiveness Applications include lung ultrasound imaging (<a href="https://pwang.pw/lungNO/" rel="external nofollow noopener" target="_blank">arXiv‚Äô24</a>) and assistive diagnosis (<a href="https://danielchyeh.github.io/MDPipe/" rel="external nofollow noopener" target="_blank">MICCAI‚Äô24</a>).</li> </ol> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 02, 2024</th> <td> <a href="https://arxiv.org/abs/2411.10919" rel="external nofollow noopener" target="_blank">SSL for Surgery</a> won the best paper at ML4H 2024! üèÜ </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 25, 2024</th> <td> <a href="https://arxiv.org/abs/2410.16290" rel="external nofollow noopener" target="_blank">Unified Model for MRI</a> is on arXiv. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 01, 2024</th> <td> <a href="./trajSSL">Pose-Aware Self-Supervised Learning</a> accepted to ECCV 2024 as an oral presentation! üéä </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 29, 2024</th> <td> <a href="https://danielchyeh.github.io/MDPipe/" rel="external nofollow noopener" target="_blank">INSIGHT</a> accepted to MICCAI 2024! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 19, 2023</th> <td> <a href="https://proceedings.mlr.press/v225/kocielnik23a.html" rel="external nofollow noopener" target="_blank">Deep Multimodal Fusion</a> won the best paper at ML4H 2023! üèÜ </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>CVPR</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/orth-480.webp 480w,/assets/img/publication_preview/orth-800.webp 800w,/assets/img/publication_preview/orth-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/orth.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="orth.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2020orthogonal" class="col-sm-8"> <div class="title">Orthogonal Convolutional Neural Networks</div> <div class="author"> <em>Jiayun Wang</em>,¬†<a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>,¬†<a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a>,¬†and¬†<a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1911.12207.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/GVT8syBZXOo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://blog.csdn.net/MTandHJ/article/details/105716261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Orthogonal-Convolutional-Neural-Networks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ocnn_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/ocnn_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/ocnn.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Deep convolutional neural networks are hindered by training instability and feature redundancy towards further performance improvement. A promising solution is to impose orthogonality on convolutional filters. We develop an efficient approach to impose filter orthogonality on a convolutional layer based on the doubly block-Toeplitz matrix representation of the convolutional kernel instead of using the common kernel orthogonality approach, which we show is only necessary but not sufficient for ensuring orthogonal convolutions. Our proposed orthogonal convolution requires no additional parameters and little computational overhead. This method consistently outperforms the kernel orthogonality alternative on a wide range of tasks such as image classification and inpainting under supervised, semi-supervised and unsupervised settings. Further, it learns more diverse and expressive features with better training stability, robustness, and generalization. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rpg-480.webp 480w,/assets/img/publication_preview/rpg-800.webp 800w,/assets/img/publication_preview/rpg-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/rpg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rpg.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2023compact" class="col-sm-8"> <div class="title">Compact and Optimal Deep Learning with Recurrent Parameter Generators</div> <div class="author"> <em>Jiayun Wang</em>,¬†<a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>,¬†<a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>,¬†<a href="https://briancheung.github.io/" rel="external nofollow noopener" target="_blank">Brian Cheung</a>,¬†and¬†<a href="https://yann.lecun.com/" rel="external nofollow noopener" target="_blank">Yann LeCun</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2107.07110.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/9S-mad1jExk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://dailyai.github.io/2021-07-16/2107-07110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Recurrent-Parameter-Generators" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/rpg_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/rpg_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/rpg.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of arbitrary architecture, in one-stage end-to-end learning. Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with faster convergence. Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18‚Äôs performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34‚Äôs performance with only 0.25% DoF! Our work shows a significant potential of constrained neural optimization in compact and optimal deep learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>ECCV</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/traj-480.webp 480w,/assets/img/publication_preview/traj-800.webp 800w,/assets/img/publication_preview/traj-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/traj.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="traj.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="wang2024pose" class="col-sm-8"> <div class="title">Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization</div> <div class="author"> <em>Jiayun Wang</em>,¬†<a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>,¬†and¬†<a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2403.14973" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=vLVclmeQxNE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://cse.engin.umich.edu/stories/helping-machine-learning-models-identify-objects-in-any-pose" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Trajectory-Regularization-Enhances-Self-Supervised-Geometric-Representation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/poseSSL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/poseSSL_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/trajSSL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper was selected as an oral presentation (2.3%).</p> </div> <div class="abstract hidden"> <p>Learning visual features from unlabeled images has proven successful for semantic categorization, often by mapping different views of the same object to the same feature to achieve recognition invariance. However, visual recognition involves not only identifying what an object is but also understanding how it is presented. For example, seeing a car from the side versus head-on is crucial for deciding whether to stay put or jump out of the way. While unsupervised feature learning for downstream viewpoint reasoning is important, it remains under-explored, partly due to the lack of a standardized evaluation method and benchmarks. We introduce a new dataset of adjacent image triplets obtained from a viewpoint trajectory, without any semantic or pose labels. We benchmark both semantic classification and pose estimation accuracies on the same visual feature. Additionally, we propose a viewpoint trajectory regularization loss for learning features from unlabeled image triplets. Our experiments demonstrate that this approach helps develop a visual representation that encodes object identity and organizes objects by their poses, retaining semantic classification accuracy while achieving emergent global pose awareness and better generalization to novel objects.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> ¬© Copyright 2025 Jiayun (Peter) Wang. Last updated: January 15, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>