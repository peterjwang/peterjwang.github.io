<!DOCTYPE html><html><head>
    <!-- Google tag (gtag.js) -->
    <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XB3PR2Y1TQ"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-XB3PR2Y1TQ');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no">
    <title>Trajectory Regularization Enhances Self-Supervised Geometric Representation</title>
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="css/Highlight-Clean.css">
    <link rel="stylesheet" href="css/styles.css">

    <!-- <link rel="apple-touch-icon" sizes="180x180" href="./assets/apple-touch-icon.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="32x32" href="./favicon-32x32.png"> -->
    <!-- <link rel="icon" type="image/png" sizes="16x16" href="./favicon-16x16.png"> -->
    <link rel="apple-touch-icon" sizes="180x180" href="./assets/table-crop.png">
    <link rel="icon" type="image/png" sizes="32x32" href="./assets/table-crop.png">
    <link rel="manifest" href="./site.webmanifest">

    <meta property="og:site_name" content="Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization">
    <meta property="og:type" content="video.other">
    <meta property="og:title" content="Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization ">
    <meta property="og:description" content="Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization, ECCV Oral 2024.">
    <meta property="og:url" content="https://VirtualPets.github.io/">
    <meta property="og:image" content="https://VirtualPets.github.io/assets/images/dreamfusion_samples.png">

    <meta property="article:publisher" content="http://pwang.pw/trajSSL/">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization">
    <meta name="twitter:description" content="We capture two aspects of object recognition through SSL (self-supervised learning): what the object is and how the object is presented.">
    <meta name="twitter:url" content="http://pwang.pw/trajSSL/">
    <meta name="twitter:image" content="https://dreamfusion3d.github.io/assets/images/dreamfusion_samples.png">
    <!-- <meta name="twitter:site" content="" /> -->

    <script src="js/video_comparison.js"></script>
    <script type="module" src="js/model-viewer.min.js"></script>
</head>

<body>
    <!-- <div class="banner">
      <video class="video lazy"
          poster="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.jpg"
          autoplay loop playsinline muted>
        <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/banner_1x6_customhue_A.mp4" type="video/mp4"></source>
      </video>
    </div> -->

    <div class="highlight-clean" style="padding-bottom: 10px;">
        <!-- <div class="container" style="max-width: 768px;"> -->
        <div class="container" style="max-width: 768px;">
            <h1 class="text-center"><b>Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization</b></h1>
        </div>



        <div class="container" style="max-width: 1000px; position: relative; left: 90px;">
            <div class="row authors">
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="http://pwang.pw/">Jiayun Wang</a></h5>
                    <h6 class="text-center">UC Berkeley &amp; Caltech</h6>
                </div>     
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://yubeichen.com/">Yubei Chen</a></h5>
                    <h6 class="text-center">UC Davis</h6>
                </div>
                <div class="col-sm-3">
                    <h5 class="text-center"><a class="text-center" href="https://web.eecs.umich.edu/~stellayu/">Stella X. Yu</a></h5>
                    <h6 class="text-center">UC Berkeley &amp; U Michigan</h6>
                </div>         
            </div>
        </div>



        <div class="container" style="max-width: 768px;">
          <h4 class="text-center">ECCV 2024 (Oral)</h4>
        </div>

        <div class="buttons" style="margin-bottom: 8px;">
            <a class="btn btn-light" role="button" href="https://arxiv.org/abs/2403.14973">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>
                Paper
            </a>
            <a class="btn btn-light" role="button" href="https://github.com/samaonline/Trajectory-Regularization-Enhances-Self-Supervised-Geometric-Representation">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                  <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path>
                </svg>
                Code
            </a>
            <a class="btn btn-light" role="button" href="https://drive.google.com/file/d/1Cq1RsNzA47qd2szqAD5nTqsSlIlzhWE9/view?usp=drive_link">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" viewBox="0 0 24 24">
                    <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z"></path>
                </svg>
                Dataset
            </a>
            <a class="btn btn-light" role="button" href="../res/pdf/poseSSL_poster.pdf">
                <svg style="width:24px;height:24px;margin-left:-12px;margin-right:12px" width="24px" height="24px" viewBox="0 0 375 531">
                    <polygon stroke="#000000" points="0.5,0.866 459.5,265.87 0.5,530.874 "></polygon>
                </svg>
                Poster
            </a>
        </div>
    </div>
    <!-- <hr class="divider" /> -->

    <div class="container" style="max-width: 768px;">
        <div class="row captioned_videos">
            <div class="col-md-12">
              <!-- <h2>VirtualPets - Overview</h2> -->
                <!-- Large format devices -->
                <!-- <video class="video lazy d-none d-xs-none d-sm-block" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/wipe_opposite_6x4_smoothstep.mp4" type="video/mp4"></source>
                </video> -->
                <!-- Small format devices -->
                <!-- <video class="video lazy d-xs-block d-sm-none" autoplay loop playsinline muted poster="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.jpg">
                    <source data-src="https://dreamfusion-cdn.ajayj.com/sept28/shaded_3x3_smoothstep.mp4" type="video/mp4"></source>
                </video> -->

                <!-- <video class="video lazy"
                  poster="./assets/1-teaser-v3-out-1.png"
                  autoplay loop playsinline muted>
                  <source data-src="./assets/teaser-vid.mp4" type="video/mp4"></source>
                </video> -->

                <!-- <video autoplay loop muted playsinline width="80%" style="display: block; margin: auto"> -->
                <!-- <video autoplay loop muted playsinline width="0%"> -->
                  <!-- <source src="./assets/teaser-vid-v1.mp4" type="video/mp4"> -->
                  <!-- <source src="./assets/teaser-vid-v2.mp4" type="video/mp4"> -->
                  <!-- <source src="./assets/1-teaser-v2.jpg" type="video/mp4"> -->
                <!-- </video> -->
<style>
  img {
    width: 100%; /* Make the image take up 100% of its container's width */
    max-width: 768px; /* Limit the maximum width of the image */
    height: auto; /* Maintain aspect ratio */
  }

  /* Optional: Adjust the width for smaller screens using a media query */
  @media (max-width: 768px) {
    img {
      max-width: 100%; /* Adjusts the image to fit within smaller screens */
    }
  }
</style>
                <img src="images/teaser.png" style="max-width: 768px;">

                <!-- <p> <b>VirtualPets</b> is a diffusion-based 3D shape generator. It enables various applications.
                  (left) VirtualPets can generate 3D shapes conditioned on different input modalities, including partial shapes, images, and text. 
                  VirtualPets can even jointly handle multiple conditioning modalities while controlling the  strength for each of them.
                  (right) We showcase an application where we leverage pretrained 2D models to texture 3D shapes generated by VirtualPets.</p> -->
        <p> <font style="font-weight: bold;"> We capture two aspects of object recognition through SSL (self-supervised learning): what the
object is and how the object is presented. </font>  Our training data are unlabeled image triplets with small pose changes from viewpoint trajectories, without any semantic or pose labels.
      </p>
              <!--   <h6 class="caption" style="text-align: center;"> <font style="font-weight: bold;">Traj SSL.&nbsp;</font> Our goal is to capture two aspects of object recognition through SSL: what the
object is and how the object is presented.  We learn SSL representations that not only capture
object semantics but also pose.Our training data are unlabeled image triplets with small pose changes from viewpoint trajectories, without any semantic or pose labels. -->
                </h6>
            </div>
        </div>
    </div>

    <!-- <br> -->


    <div class="container" style="max-width: 768px;">

        <div class="row">
            <div class="col-md-12">
                <video class="video lazy" controls="" muted="" poster="./still.jpeg">
                    <source data-src="./presentation.mp4" type="video/mp4" src="./presentation.mp4">
                </video>
            </div>
        </div>
    </div>


        <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Abstract</h2>
              <p>
                  <!-- <strong> -->
                <font>
                Learning visual features from unlabeled images has proven
successful for semantic categorization, often by mapping different views
of the same object to the same feature to achieve recognition invariance.
However, visual recognition involves not only identifying <i>what</i> an object
is but also understanding <i>how</i> it is presented. For example, seeing a car
from the side versus head-on is crucial for deciding whether to stay put or
jump out of the way. While unsupervised feature learning for downstream
viewpoint reasoning is important, it remains under-explored, partly due
to the lack of a standardized evaluation method and benchmarks.</p>
<p><font>We introduce a new dataset of adjacent image triplets obtained from a
viewpoint trajectory, without any semantic or pose labels. We benchmark both semantic classification and pose estimation accuracies on the
same visual feature. Additionally, we propose a viewpoint trajectory regularization loss for learning features from unlabeled image triplets. Our
experiments demonstrate that this approach helps develop a visual representation that encodes object identity and organizes objects by their
poses, retaining semantic classification accuracy while achieving emergent global pose awareness and better generalization to novel objects.
                </font>
                  <!-- </strong> -->
              </p>
          </div>
      </div>
  </div>

    <!-- <hr class="divider" />

    <div class="container" style="max-width: 768px;">
      <div class="row">
        <div class="col-md-12">
            <h2>3D print</h2>
        </div>
      </div>

      <video autoplay loop muted playsinline width="20%">
        <source src="./assets/3d_print.mp4" type="video/mp4">
      </video>
    </div> -->

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2> SSL for Both Object Semantics and Pose </h2>
              <!-- TODO: change samples!!! -->
          </div>
      </div>

      <img src="images/results.png" style="max-width: 768px;">

      <!-- <div class="container" style="max-width: 768px; position: relative; left: 0px;">
        <div class="row legends">
          <h6 class="text-left" style="position: relative; left: 25px;">Input</h6>
          <h6 class="text-left" style="position: relative; left: 60px;">Multimodal Shape Completion</h6>
          <h6 class="text-left" style="position: relative; left: 100px;">Input</h6>
          <h6 class="text-left" style="position: relative; left: 140px;">Multimodal Shape Completion</h6>
        </div>
      </div> -->

      <p> <font style="font-weight: bold;"> SSL representations that capture both object semantics and pose. </font> The learned representations are expected to discriminate different object semantics and
poses, achieving high accuracies for both semantic classification and pose estimation.
Notably, we expect to understand global pose from local pose changes.
        <font style="font-style: italic;"> (Left) </font> Our method excels at both semantic
classification and pose estimation over existing methods.
        <font style="font-style: italic;"> (Right) </font>  
      </p>
    </div>

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Benchmark Dataset</h2>
          </div>
      </div>

      <img src="images/dataset.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> We provide benchmark with a dataset for joint learning of object semantics and pose. </font> For semantics, we use non-overlapping 13 <span style="color:blue;">in-domain</span> semantic categories and 11 <span style="color:red;">out-of-domain</span> categories. We project in-domain and out-of-domain semantic classes with
PCA-projected Word2Vec features.
        <font style="font-style: italic;"> (Left) </font> For pose, we adopt absolute and relative pose estimation as tasks. Notably, relative pose
enables SSL’s generalizability test on out-of-domain data as it eliminates the need for
category-specific canonical pose.
        <font style="font-style: italic;"> (Right) </font>  
  
      </p>

    </div>

        <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Method: SSL with Trajectory Loss</h2>
          </div>
      </div>

      <img src="images/method.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> In addition to invariance SSL loss (e.g. SimCLR), we propose trajectory loss to avoid representation collapse on pose. </font> We enforce representations of adjacent views
of an object, z_L, z_C, z_R, to form a geodesic trajectory. 
      </p>

    </div>

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Results: Better Generalization</h2>
          </div>
      </div>

      <img src="images/r2.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;"> Our trajectory regularization consistently achieves higher relative pose estimation accuracy </font> for <span style="color:blue;">in-domain</span> and <span style="color:red;">out-of-domain</span> categories and in-domain, outof-domain poses. The SSL is on par or even outperforms supervised methods on out-of-domain data.
  
      </p>

    </div>

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Visualizing the Learned Representation</h2>
          </div>
      </div>

      <img src="images/feature.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;">  We learn joint semantic-pose embedding: Images are clustered by semantics; within
each semantic cluster, images form mini-cluster by pose </font>  Representation  is
grouped by different semantic categories. Images with the same semantic categories
form clusters. 
        <font style="font-style: italic;"> (Left) </font> Zooming in one category, airplane, we visualize 200 instances with
different poses. As the azimuth changes, their representation also forms a trajectory.
        <font style="font-style: italic;"> (Right) </font>  
      </p>


  
    </div>

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Representation for a Single-Category (Airplanes)</h2>
          </div>
      </div>

      <img src="images/feature2.png" style="max-width: 768px;">

      <p> <font style="font-weight: bold;">  Embedding of renderings of multiple airplanes with pose
changes, which demonstrates the improved representation of our method <font style="font-style: italic;"> (Left) </font>  over <a href="https://arxiv.org/abs/2105.04906">baseline (VICReg)</a>  <font style="font-style: italic;"> (Right) </font>  . </font>  In each figure, different dots refer to different airplanes with the
same pose. We observe as airplane poses change , their
representations form a trajectory in the feature space. While the baseline method
without trajectory loss can differentiate some views, it fails to form a trajectory, which
could partially contribute to worse pose estimation performance.
       
      </p>

    <hr class="divider">
   
    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
              <h2>Citation</h2>
              <code>
                  @inproceedings{wang2024pose,<br>
                    &nbsp; title  = {Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization},<br>
                    &nbsp; author={Wang, Jiayun and Chen, Yubei and Yu, Stella},<br>
                    &nbsp;   booktitle={European Conference on Computer Vision},<br>
                    &nbsp; year={2024},<br>
              }</code>
          </div>
      </div>
  </div>


<!--     <hr class="divider" />
    <div class="container" style="max-width: 768px;">
        <div class="row">
            <div class="col-md-12">
                <h2>Citation</h2>
                <code>
                    @article{cheng2022VirtualPets,<br>
                      author = {Cheng, Yen-Chi and Lee, Hsin-Ying and Tuyakov, Sergey and Schwing, Alex and Gui, Liangyan},<br>
                      title  = {{VirtualPets}: Multimodal 3D Shape Completion, Reconstruction, and Generation},<br>
                      journal = {arXiv},<br>
                      year   = {2022},<br>
                }</code>
            </div>
        </div>
    </div> -->

    <hr class="divider">

    <div class="container" style="max-width: 768px;">
      <div class="row">
          <div class="col-md-12">
            <h2>Acknowledgement</h2>
          </div>
          <h6> This project was supported, in part, by NSF 2215542, NSF 2313151, and Bosch
gift funds to S. Yu at UC Berkeley and the University of Michigan. The authors
thank Zezhou Cheng and Quentin Garrido for helpful discussions.</h6>
      </div>
    </div>

    <script src="https://polyfill.io/v3/polyfill.js?features=IntersectionObserver"></script>
    <script src="/assets/js/yall.js"></script>
    <script>
        yall(
            {
                observeChanges: true
            }
        );
    </script>
    <script src="/assets/js/scripts.js"></script>
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.bundle.min.js"></script>
    <script src="js/webflow.fd002feec.js"></script>
    <!-- Import the component -->



</div></body></html>