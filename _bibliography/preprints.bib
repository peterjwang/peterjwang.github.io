
@article{wang2024lung,
  author    = {Jiayun Wang and Oleksii Ostras and Masashi Sode and Bahareh Tolooshams and Zongyi Li and Kamyar Azizzadenesheli and Giammarco Pinton and Anima Anandkumar},
  title     = {Ultrasound Lung Aeration Map via Physics-Aware Neural Operators},
  journal   = {In submission to Nature},
  year      = {2025},
  abbr={arXiv},
       preview={lusno.png},
         website      = {https://pwang.pw/lungNO/},
  pdf          = {https://drive.google.com/file/d/1M91eHRe10B0O65JokI0LoaVt6ctlhHpK/view?usp=sharing},
  selected     = {true},
    abstract     = {Diagnostic lung ultrasound (LUS) is common in clinics in the evaluation of acute and chronic lung diseases. Ultrasound transducer emits diagnostic pulse, receives pressure waves at its surface, transforms them into radio frequency (RF) data and ultrasound machine beamforms them to B-mode images, which are read by radiologists for diagnostic purposes and monitoring of lung disease. However, interpreting LUS B-mode images is challenging due to the ill-posedness of the problem, which comes from ultrasoundâ€™s inability to penetrate air and the complex physics of wave propagation in the lung. The interpretation difficulty consequently reduces the repeatability and increases the variance in LUS-based diagnosis and monitoring. In this work, we propose a surrogate model for the inverse problem of ultrasound propagation in the lung and reconstruct lung aeration maps (the source) directly from the ultrasound RF data, which avoids the indirect and challenging interpretation of the B-mode images.},
}

article{wang2024fast,
  author    = {Jiayun Wang and Yousuf Aborahama and Julius Berner and Zongyi Li and Kamyar Azizzadenesheli and Lihong V. Wang and Anima Anandkumar},
  title     = {Fast and Resolution-Invariant 3D Photoacoustic Computed Tomography via Operator Learning},
  journal   = {Nature Communications (in submission)},
  year      = {2024},
  abbr={arXiv}
}

@inproceedings{wang2024beyond,
  title={Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators},
  author={Wang, Chuwei and Berner, Julius and Li, Zongyi and Zhou, Di and Wang, Jiayun and Bae, Jane and Anandkumar, Anima},
  booktitle={submission to ICLR},
  year={2025},
  abbr={arXiv},
       preview={closure.png},
       pdf={https://arxiv.org/pdf/2408.05177},
  html={https://arxiv.org/abs/2408.05177},
  abstract={Accurately predicting the long-term behavior of chaotic systems is crucial for various applications such as climate modeling. However, achieving such predictions typically requires iterative computations over a dense spatiotemporal grid to account for the unstable nature of chaotic systems, which is expensive and impractical in many real-world situations. An alternative approach to such a full-resolved simulation is using a coarse grid and then correcting its errors through a \textit{closure model}, which approximates the overall information from fine scales not captured in the coarse-grid simulation. Recently, ML approaches have been used for closure modeling, but they typically require a large number of training samples from expensive fully-resolved simulations (FRS). In this work, we prove an even more fundamental limitation, i.e., the standard approach to learning closure models suffers from a large approximation error for generic problems, no matter how large the model is, and it stems from the non-uniqueness of the mapping. We propose an alternative end-to-end learning approach using a physics-informed neural operator (PINO) that overcomes this limitation by not using a closure model or a coarse-grid solver.},
}

@inproceedings{Yao2024open,
  author    = {Jin Yao and Hao Gu and Xuweiyi Chen and Jiayun Wang and Zezhou Cheng},
  title     = {Open Vocabulary Monocular 3D Object Detection},
  booktitle={submission},
  year      = {2025},
  abbr={arXiv},
       preview={3ddet.jpg},
  pdf          = {https://arxiv.org/pdf/2411.16833},
  html          = {https://arxiv.org/abs/2411.16833},
  website      = {https://uva-computer-vision-lab.github.io/ovmono3d/},
  code         = {https://github.com/UVA-Computer-Vision-Lab/ovmono3d},
  abstract     = {In this work, we pioneer the study of open-vocabulary monocular 3D object detection, a novel task that aims to detect and localize objects in 3D space from a single RGB image without limiting detection to a predefined set of categories. We formalize this problem, establish baseline methods, and introduce a class-agnostic approach that leverages open-vocabulary 2D detectors and lifts 2D bounding boxes into 3D space. Our approach decouples the recognition and localization of objects in 2D from the task of estimating 3D bounding boxes, enabling generalization across unseen categories. Additionally, we propose a target-aware evaluation protocol to address inconsistencies in existing datasets, improving the reliability of model performance assessment. Extensive experiments on the Omni3D dataset demonstrate the effectiveness of the proposed method in zero-shot 3D detection for novel object categories, validating its robust generalization capabilities. Our method and evaluation protocols contribute towards the development of open-vocabulary object detection models that can effectively operate in real-world, category-diverse environments.},
}




%not included
%SSL PDE
%fus