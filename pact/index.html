<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Accelerating 3D Photoacoustic Computed Tomography with Physics-Aware Neural Operators</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://pwang.pw/" target="_blank">Jiayun Wang</a>,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/yousuf-aborahama-85b5a6256" target="_blank">Yousuf Aborahama</a>,</span>
                  <span class="author-block">
                     <span class="author-block">
                    <a href="https://www.linkedin.com/in/arya-khokhar" target="_blank">Arya Khokhar</a>,
                  </span>
                                      <span class="author-block">
                    <a href="https://www.zhangcv.com/" target="_blank">Yang Zhang</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://cwwangcal.github.io/" target="_blank">Chuwei Wang</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=0kVF2G4AAAAJ&hl=en" target="_blank">Karteekeya Sastry</a>,
                  </span>
                                    <span class="author-block">
                     <span class="author-block">
                    <a href="https://jberner.info/" target="_blank">Julius Berner</a>,
                  </span>
                  <span class="author-block">
                    <a href="http://elynluo.github.io/" target="_blank">Yilin Luo</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://bonevbs.github.io/" target="_blank">Boris Bonev</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://zongyi-li.github.io/" target="_blank">Zongyi Li</a>,
                  </span><br>
                  <span class="author-block">
                    <a href="https://kamyar.page/" target="_blank">Kamyar Azizzadenesheli</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://mede.caltech.edu/people/lvw" target="_blank">Lihong V. Wang</a><sup>*</sup>,
                  </span>
                                    <span class="author-block">
                    <a href="http://tensorlab.cms.caltech.edu/users/anima/" target="_blank">Anima Anandkumar</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Caltech and NVIDIA<br>In submission</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Advising</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://drive.google.com/file/d/1nmp9cPtgWc-AR6Fn8YNmkR7udVIN1qYn/view?usp=sharing" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/pact_release_slides.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Slides</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Photoacoustic computed tomography (PACT) combines optical contrast with ultrasonic resolution,
achieving deep-tissue imaging beyond the optical diffusion limit. While three-dimensional PACT
systems enable high-resolution volumetric imaging for applications spanning transcranial to breast
imaging, current implementations require dense transducer arrays and prolonged acquisition times,
limiting clinical translation. We introduce Pano (PACT imaging neural operator), an end-to-end
physics-aware model that directly learns the inverse acoustic mapping from sensor measurements
to volumetric reconstructions. Unlike existing approaches (e.g. universal back-projection algorithm),
Pano learns both physics and data priors while also being agnostic to the input data resolution.
Pano employs spherical discrete-continuous convolutions to preserve hemispherical sensor geometry,
incorporates Helmholtz equation constraints to ensure physical consistency and operates resolutionindependently across varying sensor configurations. We demonstrate the robustness and efficiency of
Pano in reconstructing high-quality images from both simulated and real experimental data, achieving
consistent performance even with significantly reduced transducer counts and limited-angle acquisition
configurations. The framework maintains reconstruction fidelity across diverse sparse sampling patterns
while enabling real-time volumetric imaging capabilities. This advancement establishes a practical
pathway for making 3D PACT more accessible and feasible for both preclinical research and clinical
applications, substantially reducing hardware requirements without compromising image reconstruction
quality.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{wang2025ultrasound,
  title={Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators},
  author={Wang, Jiayun and Aborahama, Yousuf and Zhang, Yang and Berner, Julius and Li, Zongyi and Azizzadenesheli, Kamyar and Wang, Lihong V. and Anandkumar, Anima},
  year={2025}
}
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Acknowledgment: This work is supported in part by ONR (MURI grants N000142312654 and
N000142012786) and the United States National Institutes of Health (NIH) grants U01 EB029823 (BRAIN
Initiative), R35 CA220436 (Outstanding Investigator Award), and R01 CA282505. J.W. is supported in
part by the Pritzker AI+Science initiative and Schmidt Sciences. A.A. is supported in part by the Bren
endowed chair and the AI2050 senior fellow program at Schmidt Sciences. B. T. is supported in part by
the Swartz Foundation Fellowship. Z. Li is supported in part by the NVIDIA Fellowship. The authors
thank Rui Cao for helpful discussions.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
