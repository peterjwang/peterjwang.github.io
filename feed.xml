<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://peterjwang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://peterjwang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-22T07:39:57+00:00</updated><id>https://peterjwang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Peter Wang&apos;s Homepage </subtitle><entry><title type="html">Recruiting Fall 2026 PhD Students @ Georgia Tech CSE ‚Äî Machine Learning, Computer Vision and AI for Science</title><link href="https://peterjwang.github.io/blog/2025/phdr/" rel="alternate" type="text/html" title="Recruiting Fall 2026 PhD Students @ Georgia Tech CSE ‚Äî Machine Learning, Computer Vision and AI for Science"/><published>2025-10-20T00:00:00+00:00</published><updated>2025-10-20T00:00:00+00:00</updated><id>https://peterjwang.github.io/blog/2025/phdr</id><content type="html" xml:base="https://peterjwang.github.io/blog/2025/phdr/"><![CDATA[<p>Hi, I‚Äôm Jiayun (Peter) Wang üëã. I will join <a href="https://cse.gatech.edu/">Georgia Tech CSE</a> as an Assistant Professor in 2026. I am actively recruiting PhD students starting Fall 2026.</p> <p>üîó PI website: <a href="https://pwang.pw/">https://pwang.pw/</a></p> <p>üîó Lab website: TBA</p> <h2 id="background">Background</h2> <p>I am currently a senior research scientist at Accenture GenAI, working on multimodal LLMs and agents. I am also affiliated with Caltech, where I did my postdoc on physical AI, working with <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>. Before joining Caltech, I completed my Ph.D. at UC Berkeley on computer vision, advised by Prof. <a href="https://web.eecs.umich.edu/~stellayu/">Stella Yu</a>. My research lies at the intersection of machine learning, computer vision, and AI for medicine/science. Some recent research highlights:</p> <ul> <li>ML with minimal human supervision ‚Äì self-supervised learning from unlabeled data for recognition, detection and geometry.</li> <li>Efficient AI algorithms ‚Äì theory-driven model designs leveraging data structures like orthogonality and recurrence.</li> <li>AI for Medicine and Science ‚Äì multi-modal diagnosis with LLMs and computational imaging.</li> </ul> <h2 id="recruiting-information">Recruiting Information</h2> <p>I actively look for PhD students for Fall 2026 at Georgia Tech CSE. The school of CSE participates in five <a href="https://cse.gatech.edu/phd-programs">Ph.D. programs</a>. I will mainly consider students for the following PhD programs: CSE, CS and ML. If you are interested in building next-generation machine learning and vision models and/or applying AI to biomedical and scientific problems, I‚Äôd love to hear from you! Please also <a href="https://grad.gatech.edu/admissions">apply to corresponding PhD programs</a>.</p> <h2 id="future-lab-directions">Future Lab Directions</h2> <h3 id="1Ô∏è‚É£-core-ml-and-computer-vision-methods">1Ô∏è‚É£ Core ML and Computer Vision Methods</h3> <ul> <li>Multi-modality, e.g. vision+X, multimodal LLMs</li> <li>Generative models and representation learning</li> <li>3D vision, world model, VLA, physical AI</li> <li>Physics-aware ML and simulation-based learning</li> </ul> <h3 id="2Ô∏è‚É£-ai-for-medicine--science">2Ô∏è‚É£ AI for Medicine / Science</h3> <ul> <li>Inverse problems, computational imaging and medical imaging</li> <li>Multimodal data integration, e.g. electronic health records (EHRs)</li> <li>AI+simulation, physics and other scientific applications</li> </ul> <h2 id="why-join-us">Why Join Us</h2> <ul> <li>Work at the intersection of core ML, computer vision and real-world impact.</li> <li>PI has abundant experience in real-world applications, physical AI/sim2real, interdisciplinary collaboration and mentoring experience.</li> <li>Collaborate across Georgia Tech‚Äôs AI, computing and science ecosystems.</li> <li>Get hands-on experience in both theoretical ML and applied domains like computational imaging and real-world applications.</li> <li>Engage with a growing network of collaborators at Caltech, Berkeley and beyond.</li> <li>Strong connections with industry, supporting students for internships, co-advising, and industrial collaborations.</li> <li>Abundant computing and infrastructure (lab has its own 8x Blackwell 6000 96GB GPU, A100/H100 GPU server; gatech HPC like <a href="https://pace.gatech.edu/">PACE</a>).</li> </ul> <p>If you‚Äôre passionate about developing foundational AI models that can understand, generate, and discover across modalities and scientific frontiers ‚Äî you‚Äôre warmly welcome to apply to Georgia Tech CSE!</p> <p>üì© Feel free to reach out <a href="mailto:gtlvs.info@gmail.com">gtlvs.info@gmail.com</a> (a specific email designed for collecting information) with your CV, transcript and a short note about your research interests.</p>]]></content><author><name></name></author><category term="phd"/><category term="gatech"/><category term="GeorgiaTech"/><category term="CSE"/><category term="AI"/><summary type="html"><![CDATA[phd recruting]]></summary></entry><entry><title type="html">Large-Scale Long-Tailed Recognition in an Open World</title><link href="https://peterjwang.github.io/blog/2019/ltor/" rel="alternate" type="text/html" title="Large-Scale Long-Tailed Recognition in an Open World"/><published>2019-05-13T00:00:00+00:00</published><updated>2019-05-13T00:00:00+00:00</updated><id>https://peterjwang.github.io/blog/2019/ltor</id><content type="html" xml:base="https://peterjwang.github.io/blog/2019/ltor/"><![CDATA[<p>This is a blog post for <a href="https://arxiv.org/abs/1904.05160">Large-Scale Long-Tailed Recognition in an Open World</a>, a CVPR 19 paper.</p> <h2 id="existing-computer-vision-setting-vs-real-world-scenario">Existing Computer Vision Setting v.s. Real-World Scenario</h2> <p>One day, an ecologist came to us. He wanted to use modern computer vision techniques to perform automatic animal identification in his wildlife camera trap image datasets. We were so confident because it sounded just like a basic image classification problem. However, we failed. The dataset he provided was extremely long-tailed and open-ended. As usual, when we did not have enough training data, we asked if it was possible to provide more data for the tail classes and just ignore the open classes that might appear in the testing dataset. Unfortunately, collecting more data was not the option. It could take an extremely long time for these ecologists to take photos of rare and secluded animals in the wild. For some endangered animals, they even had to wait for years for one single shot. At the same time, new animal species kept coming in, and old animal species kept leaving. The total class number was never fixed in such a dynamic system. Moreover, the identification of rare and new animals has more conservational values than abundant animals. If we could only do well on the abundant classes, the method would never be practically usable. We tried all possible methods we could think of (data augmentation, sampling techniques, few-shot learning, imbalanced classification, etc.); but none of the existing methods could handle abundant classes, scarce classes and open classes at the same time.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ltor-teaser-480.webp 480w,/assets/img/ltor-teaser-800.webp 800w,/assets/img/ltor-teaser-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/ltor-teaser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>There exists a considerable gap between the existing computer vision setting and the real-world scenario.</p> <p>Since then, we have been thinking, <strong>what is the biggest reason for this gap between existing computer vision methods and real-world scenarios?</strong> Similar situations don‚Äôt just happen in wildlife image data, they happen over and over again in real-world scenarios (both in the industry and in the academia). If convolutional neural networks can classify images from the massive ImageNet dataset so well, why image classification is still an unsolved problem in an open world? Almost every task (e.g. few-shot learning and open set recognition) proposed in visual recognition field has successful approaches, but it seems that no one has tried to see those problems as a whole. When it comes to real-world applications, classification tasks (either for head classes or for tail classes) sometimes do not just come alone. Therefore, we think that the gap may come from the problem setting of visual recognition itself.</p> <h2 id="open-long-tailed-recognition-oltr">Open Long-Tailed Recognition (OLTR)</h2> <p>In existing visual recognition setting, the training data and testing data are both balanced under a closed-world setting, e.g. the ImageNet dataset. However, this setting is not a good proxy of the real-world scenario. For example, it is never possible for ecologists to gather balanced wildlife datasets because animal distribution is imbalanced. Similarly, people are bothered by the imbalanced and open-ended distribution from all sorts of datasets: street signs, fashion brands, faces, weather conditions, street conditions, etc. To faithfully reflect these aspects, we formally study <strong>‚ÄúOpen Long-Tailed Recognition‚Äù (OLTR)</strong> arising in natural data settings. A practical system shall be able to classify among a few common and many rare categories, to generalize the concept of a single category from only a few known instances, and to acknowledge novelty upon an instance of a never seen category. We define OLTR as learning from long-tail and open-end distributed data and evaluating the classification accuracy over a balanced test set which includes head, tail, and open classes in a continuous spectrum (Fig. 2).</p> <p>While OLTR has not been defined in the literature, there are three closely related tasks which are often studied in isolation: imbalanced classification, few-shot learning, and open-set recognition. Fig. 3 summarizes their differences. The newly proposed Open Long-Tailed Recognition (OLTR) serves as a more comprehensive and more realistic touchstone for evaluating visual recognition systems.</p> <h2 id="the-importance-of-attention--memory">The Importance of Attention &amp; Memory</h2> <p>We propose to map an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our proposed dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes, as illustrated in Fig. 4.</p> <p>Firstly, we obtain a visual memory by aggregating the knowledge from both head and tail classes. Then the visual concepts stored in the memory are infused back as associated memory feature to enhance the original direct feature. It can be understood as using induced knowledge (i.e. memory feature) to assist the direct observation (i.e. direct feature). We further learn a concept selector to control the amount and type of memory feature to be infused. Since head classes already have an abundant direct observation, only a small amount of memory feature is infused for them. On the contrary, tail classes suffer from scarce observation, the associated visual concepts in memory feature are extremely beneficial. Finally, we calibrate the confidence of open classes by calculating their reachability to the obtained visual memory.</p> <h2 id="across-the-board-improvements">Across-the-board Improvements</h2> <p>As demonstrated in Fig. 5, our approach provides comprehensive treatment to all the many/medium/few-shot classes as well as the open classes, achieving substantial improvements on all aspects.</p> <h2 id="visualization-of-learning-dynamics">Visualization of Learning Dynamics</h2> <p>Here we inspect the visual concepts that memory feature has infused by visualizing its top activating neurons as shown in Fig. 6. Specifically, for each input image, we identify its top-3 transferred neurons in memory feature. And each neuron is visualized by a collection of highest activated patches over the whole training set. For example, to classify the top left image which belongs to a tail class ‚Äúcock‚Äù, our approach has learned to transfer visual concepts that represent ‚Äúbird head‚Äù, ‚Äúround shape‚Äù and ‚Äúdotted texture‚Äù respectively. After feature infusion, the dynamic meta-embedding becomes more informative and discriminative.</p> <p>Back to the Real World Now we go back to the real jungle and apply our proposed approach to the wildlife data came with the ecologist mentioned in the first section. Fortunately, our new framework achieves a substantial improvement on scarce classes without a sacrifice of the abundant classes. More specifically, we obtain around 40% performance gains (from 25% to 66%) on classes with less than 40 images. And we also obtain over 15% performance gains for open class detection.</p> <p>We believe computational methods developed under open long-tailed recognition setting can ultimately satisfy the needs of natural-distributed datasets. In summary, Open Long-Tailed Recognition (OLTR) serves as a more comprehensive and more realistic touchstone for evaluating visual recognition systems, which can be further extended into detection, segmentation and reinforcement learning.</p> <p>Acknowledgements: We thank all co-authors of the paper ‚ÄúLarge-Scale Long-Tailed Recognition in an Open World‚Äù for their contributions and discussions in preparing this blog. The views and opinions expressed in this blog are solely of the authors of this paper.</p> <p>This blog post is based on the following paper which will be presented at IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019) as an oral presentation:</p> <ul> <li>Large-Scale Long-Tailed Recognition in an Open World Ziwei Liu<em>, Zhongqi Miao</em>, Xiaohang Zhan, Jiayun Wang, Boqing Gong, Stella X. Yu <a href="https://arxiv.org/abs/1904.05160">Paper</a>, <a href="https://liuziwei7.github.io/projects/LongTail.html">Project Page</a>, <a href="https://drive.google.com/drive/folders/1j7Nkfe6ZhzKFXePHdsseeeGI877Xu1yf">Dataset</a>, <a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR">Code &amp; Model</a></li> </ul>]]></content><author><name></name></author><category term="long-tailed"/><category term="recognition"/><category term="open-set"/><category term="vision"/><summary type="html"><![CDATA[how do we deal with real-world data with strong imbalanced distribution]]></summary></entry></feed>