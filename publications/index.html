<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Jiayun (Peter) Wang </title> <meta name="author" content="Jiayun (Peter) Wang"> <meta name="description" content="Asterisk indicates equal contribution."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A9&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://peterjwang.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jiayun</span> (Peter) Wang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/misc/">Misc </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">Asterisk indicates equal contribution.</p> </header> <article> <p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=IBn7PdYAAAAJ" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h1>Preprints</h1> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lusno-480.webp 480w,/assets/img/publication_preview/lusno-800.webp 800w,/assets/img/publication_preview/lusno-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/lusno.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="lusno.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024lung" class="col-sm-8"> <div class="title">Ultrasound Lung Aeration Map via Physics-Aware Neural Operators</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://scholar.google.com/citations?user=iKpmeMcAAAAJ" rel="external nofollow noopener" target="_blank">Oleksii Ostras</a>, Masashi Sode, <a href="https://btolooshams.github.io/" rel="external nofollow noopener" target="_blank">Bahareh Tolooshams</a>, <a href="https://zongyi-li.github.io/" rel="external nofollow noopener" target="_blank">Zongyi Li</a>, <a href="https://kamyar.page/index.html" rel="external nofollow noopener" target="_blank">Kamyar Azizzadenesheli</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Giammarco Pinton, Anima Anandkumar' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In submission to Nature</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/closure-480.webp 480w,/assets/img/publication_preview/closure-800.webp 800w,/assets/img/publication_preview/closure-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/closure.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="closure.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024beyond" class="col-sm-8"> <div class="title">Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators</div> <div class="author"> <a href="https://scholar.google.com/citations?user=O5GIrl4AAAAJ" rel="external nofollow noopener" target="_blank">Chuwei Wang</a>, <a href="https://jberner.info/" rel="external nofollow noopener" target="_blank">Julius Berner</a>, <a href="https://zongyi-li.github.io/" rel="external nofollow noopener" target="_blank">Zongyi Li</a> , <a href="https://dizhou-flow.github.io/" rel="external nofollow noopener" target="_blank">Di Zhou</a> , <em>Jiayun Wang</em>, <a href="https://bae.caltech.edu/" rel="external nofollow noopener" target="_blank">Jane Bae</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Anima Anandkumar' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In submission to ICLR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2408.05177" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2408.05177" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Accurately predicting the long-term behavior of chaotic systems is crucial for various applications such as climate modeling. However, achieving such predictions typically requires iterative computations over a dense spatiotemporal grid to account for the unstable nature of chaotic systems, which is expensive and impractical in many real-world situations. An alternative approach to such a full-resolved simulation is using a coarse grid and then correcting its errors through a \textitclosure model, which approximates the overall information from fine scales not captured in the coarse-grid simulation. Recently, ML approaches have been used for closure modeling, but they typically require a large number of training samples from expensive fully-resolved simulations (FRS). In this work, we prove an even more fundamental limitation, i.e., the standard approach to learning closure models suffers from a large approximation error for generic problems, no matter how large the model is, and it stems from the non-uniqueness of the mapping. We propose an alternative end-to-end learning approach using a physics-informed neural operator (PINO) that overcomes this limitation by not using a closure model or a coarse-grid solver.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/3ddet-480.webp 480w,/assets/img/publication_preview/3ddet-800.webp 800w,/assets/img/publication_preview/3ddet-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/3ddet.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="3ddet.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yao2024open" class="col-sm-8"> <div class="title">Open-Vocabulary Monocular 3D Object Detection via Objectiveness Lifting</div> <div class="author"> <a href="https://yaojin17.github.io/" rel="external nofollow noopener" target="_blank">Jin Yao</a>, Hao Gu , Xuweiyi Chen , <em>Jiayun Wang</em>, and <a href="https://sites.google.com/site/zezhoucheng/" rel="external nofollow noopener" target="_blank">Zezhou Cheng</a> </div> <div class="periodical"> <em>In submission to CVPR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML4H</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sslsurg-480.webp 480w,/assets/img/publication_preview/sslsurg-800.webp 800w,/assets/img/publication_preview/sslsurg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sslsurg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sslsurg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="gutpa2024" class="col-sm-8"> <div class="title">Multi-Modal Self-Supervised Learning for Surgical Feedback Effectiveness Assessment</div> <div class="author"> Arushi Gupta<sup>*</sup>, <a href="https://www.rkocielnik.com/" rel="external nofollow noopener" target="_blank">Rafal Kocielnik<sup>*</sup></a> , <em>Jiayun Wang<sup>*</sup></em>, Firdavs Nasriddinov, Cherine Yang, Elyssa Wong, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Anima Anandkumar, Andrew J Hung' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In submission to Machine Learning for Health, PMLR</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h1>Conference &amp; Journal Articles</h1> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>ECCV</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/traj-480.webp 480w,/assets/img/publication_preview/traj-800.webp 800w,/assets/img/publication_preview/traj-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/traj.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="traj.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024pose" class="col-sm-8"> <div class="title">Pose-Aware Self-Supervised Learning with Viewpoint Trajectory Regularization</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2403.14973" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=vLVclmeQxNE" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/samaonline/Trajectory-Regularization-Enhances-Self-Supervised-Geometric-Representation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/poseSSL_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/poseSSL_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/trajSSL" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper was selected as an oral presentation (2.3%).</p> </div> <div class="abstract hidden"> <p>Learning visual features from unlabeled images has proven successful for semantic categorization, often by mapping different views of the same object to the same feature to achieve recognition invariance. However, visual recognition involves not only identifying what an object is but also understanding how it is presented. For example, seeing a car from the side versus head-on is crucial for deciding whether to stay put or jump out of the way. While unsupervised feature learning for downstream viewpoint reasoning is important, it remains under-explored, partly due to the lack of a standardized evaluation method and benchmarks. We introduce a new dataset of adjacent image triplets obtained from a viewpoint trajectory, without any semantic or pose labels. We benchmark both semantic classification and pose estimation accuracies on the same visual feature. Additionally, we propose a viewpoint trajectory regularization loss for learning features from unlabeled image triplets. Our experiments demonstrate that this approach helps develop a visual representation that encodes object identity and organizes objects by their poses, retaining semantic classification accuracy while achieving emergent global pose awareness and better generalization to novel objects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Compress @ NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/MRINO-480.webp 480w,/assets/img/publication_preview/MRINO-800.webp 800w,/assets/img/publication_preview/MRINO-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/MRINO.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="MRINO.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jatyani2024" class="col-sm-8"> <div class="title">Unifying Subsampling Pattern Variations for Compressed Sensing MRI with Neural Operators</div> <div class="author"> Armeet Jatyani<sup>*</sup> , <em>Jiayun Wang<sup>*</sup></em>, Zihui Wu, Miguel Liu-Schiaffini, <a href="https://btolooshams.github.io/" rel="external nofollow noopener" target="_blank">Bahareh Tolooshams</a>, and <a href="http://tensorlab.cms.caltech.edu/" rel="external nofollow noopener" target="_blank">Anima Anandkumar</a> </div> <div class="periodical"> <em>In NeurIPS Workshop</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">CVPR</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The full paper is in submission to CVPR 2025.</p> </div> <div class="abstract hidden"> <p>Compressed Sensing MRI (CS-MRI) reconstructs images of the body’s internal anatomy from undersampled and compressed measurements, thereby reducing scan times and minimizing the duration patients need to remain still. Recently, deep neural networks have shown great potential for reconstructing high-quality images from highly undersampled measurements. However, since deep neural networks operate on a fixed discretization, one needs to train multiple models for different measurement subsampling patterns and image resolutions. This approach is highly impractical in clinical settings, where subsampling patterns and image resolutions are frequently varied to accommodate different imaging and diagnostic requirements. We propose a unified model that is robust to different subsampling patterns and image resolutions in CS-MRI. Our model is based on neural operators, a discretization-agnostic architecture. We use neural operators in both image and measurement (frequency) space, which capture local and global image features for MRI reconstruction. Empirically, we achieve consistent performance across different subsampling rates and patterns, with up to 4x lower NMSE and 5 dB PSNR improvements over the state-of-the-art method. We also show the model is agnostic to image resolutions with zero-shot super-resolution results. Our unified model is a promising tool that is agnostic to measurement subsampling and imaging resolutions in MRI, offering significant utility in clinical settings where flexibility and adaptability are essential for efficient and reliable imaging.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thumbnail_mdpipe-480.webp 480w,/assets/img/publication_preview/thumbnail_mdpipe-800.webp 800w,/assets/img/publication_preview/thumbnail_mdpipe-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/thumbnail_mdpipe.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thumbnail_mdpipe.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yeh2024insight" class="col-sm-8"> <div class="title">Insight: A Multi-Modal Diagnostic Pipeline using LLMs for Ocular Surface Disease Diagnosis</div> <div class="author"> <a href="https://danielchyeh.github.io/" rel="external nofollow noopener" target="_blank">Chun-Hsiao Yeh</a> , <em>Jiayun Wang</em>, <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D. Graham</a> , Andrea J. Liu, Bo Tan, <a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Yi Ma, Meng C. Lin' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Medical Image Computing and Computer-Assisted Intervention (MICCAI) Proceedings</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2410.00292" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://danielchyeh.github.io/MDPipe/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Accurate diagnosis of ocular surface diseases is critical in optometry and ophthalmology, which hinge on integrating clinical data sources (e.g., meibography imaging and clinical metadata). Traditional human assessments lack precision in quantifying clinical observations, while current machine-based methods often treat diagnoses as multi-class classification problems, limiting the diagnoses to a predefined closed-set of curated answers without reasoning the clinical relevance of each variable to the diagnosis. To tackle these challenges, we introduce an innovative multi-modal diagnostic pipeline (MDPipe) by employing large language models (LLMs) for ocular surface disease diagnosis. We first employ a visual translator to interpret meibography images by converting them into quantifiable morphology data, facilitating their integration with clinical metadata and enabling the communication of nuanced medical insight to LLMs. To further advance this communication, we introduce a LLM-based summarizer to contextualize the insight from the combined morphology and clinical metadata, and generate clinical report summaries. Finally, we refine the LLMs’ reasoning ability with domain-specific insight from real-life clinician diagnoses. Our evaluation across diverse ocular surface disease diagnosis benchmarks demonstrates that MDPipe outperforms existing standards, including GPT-4, and provides clinically sound rationales for diagnoses.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Heliyon</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/parts_of_eye-480.webp 480w,/assets/img/publication_preview/parts_of_eye-800.webp 800w,/assets/img/publication_preview/parts_of_eye-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/parts_of_eye.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="parts_of_eye.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="tejas2024" class="col-sm-8"> <div class="title">A Machine Learning Approach to Predicting Dry Eye-Related Signs, Symptoms and Diagnoses</div> <div class="author"> <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D Graham</a>, <a href="https://tejasvikothapalli.github.io/" rel="external nofollow noopener" target="_blank">Tejasvi Kothapalli</a> , <em>Jiayun Wang</em>, Jennifer Ding, Vivien Tse, Penny Asbell, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Stella Yu, Meng C Lin' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Heliyon</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.cell.com/heliyon/fulltext/S2405-8440(24)12052-X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/eyeheliyon.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>To use artificial intelligence to identify relationships between morphological characteristics of the Meibomian glands (MGs), subject factors, clinical outcomes, and subjective symptoms of dry eye. A deep learning model was trained to take meibography as input, segment the individual MG in the images, and learn their detailed morphological features. Morphological characteristics were then combined with clinical and symptom data in prediction models of MG function, tear film stability, ocular surface health, and subjective discomfort and dryness. The models were analyzed to identify the most heavily weighted features used by the algorithm for predictions. Machine learning-derived MG morphological characteristics were found to be important in predicting multiple signs, symptoms, and diagnoses related to MG dysfunction and dry eye. This deep learning method illustrates the rich clinical information that detailed morphological analysis of the MGs can provide, and shows promise in advancing our understanding of the role of MG morphology in ocular surface health. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Nature Portfolio</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sreye-480.webp 480w,/assets/img/publication_preview/sreye-800.webp 800w,/assets/img/publication_preview/sreye-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sreye.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sreye.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lifestyle2024" class="col-sm-8"> <div class="title">Artificial Intelligence Models Utilize Lifestyle Factors to Predict Dry Eye Related Outcomes</div> <div class="author"> <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D. Graham</a> , <em>Jiayun Wang</em>, <a href="https://tejasvikothapalli.github.io/" rel="external nofollow noopener" target="_blank">Tejasvi Kothapalli</a>, Jennifer Ding, Helen Tasho, Alisa Molina, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Vivien Tse, Sarah Chang, Stella Yu, Meng C. Lin' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>Nature Scientific Reports</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.researchgate.net/publication/382392066_Artificial_Intelligence_Models_Utilize_Lifestyle_Factors_to_Predict_Dry_Eye-Related_Outcomes" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/MGAI.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Purpose: To examine and interpret machine learning models that predict dry eye (DE)-related clinical signs, subjective symptoms, and clinician diagnoses by heavily weighting lifestyle factors in the predictions. Methods: Machine learning models were trained to take clinical assessments of the ocular surface, eyelids, and tear film, combined with symptom scores from validated questionnaire instruments for DE and clinician diagnoses of ocular surface diseases, and perform a classification into DE-related outcome categories. Outcomes are presented for which the data-driven algorithm identified subject characteristics, lifestyle, behaviors, or environmental exposures as heavily weighted predictors. Models were assessed by 5-fold cross-validation accuracy and class-wise statistics of the predictors. Conclusions: The results emphasize the importance of lifestyle, subject, and environmental characteristics in the etiology of ocular surface disease. Lifestyle factors should be taken into account in clinical research and care to a far greater extent than has been the case to date.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WACV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rpg-480.webp 480w,/assets/img/publication_preview/rpg-800.webp 800w,/assets/img/publication_preview/rpg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/rpg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rpg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023compact" class="col-sm-8"> <div class="title">Compact and Optimal Deep Learning with Recurrent Parameter Generators</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>, <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>, <a href="https://briancheung.github.io/" rel="external nofollow noopener" target="_blank">Brian Cheung</a>, and <a href="https://yann.lecun.com/" rel="external nofollow noopener" target="_blank">Yann LeCun</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2107.07110.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/9S-mad1jExk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://dailyai.github.io/2021-07-16/2107-07110" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Recurrent-Parameter-Generators" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/rpg_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/rpg.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Deep learning has achieved tremendous success by training increasingly large models, which are then compressed for practical deployment. We propose a drastically different approach to compact and optimal deep learning: We decouple the Degrees of freedom (DoF) and the actual number of parameters of a model, optimize a small DoF with predefined random linear constraints for a large model of arbitrary architecture, in one-stage end-to-end learning. Specifically, we create a recurrent parameter generator (RPG), which repeatedly fetches parameters from a ring and unpacks them onto a large model with random permutation and sign flipping to promote parameter decorrelation. We show that gradient descent can automatically find the best model under constraints with faster convergence. Our extensive experimentation reveals a log-linear relationship between model DoF and accuracy. Our RPG demonstrates remarkable DoF reduction and can be further pruned and quantized for additional run-time performance gain. For example, in terms of top-1 accuracy on ImageNet, RPG achieves 96% of ResNet18’s performance with only 18% DoF (the equivalent of one convolutional layer) and 52% of ResNet34’s performance with only 0.25% DoF! Our work shows a significant potential of constrained neural optimization in compact and optimal deep learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ML4H</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/deepmfusion-480.webp 480w,/assets/img/publication_preview/deepmfusion-800.webp 800w,/assets/img/publication_preview/deepmfusion-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/deepmfusion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="deepmfusion.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kocielnik2023deep" class="col-sm-8"> <div class="title">Deep Multimodal Fusion for Surgical Feedback Classification</div> <div class="author"> <a href="https://www.rkocielnik.com/" rel="external nofollow noopener" target="_blank">Rafal Kocielnik</a>, Elyssa Y Wong, Timothy N Chu , Lydia Lin, <a href="https://ai.stanford.edu/~dahuang/" rel="external nofollow noopener" target="_blank">De-An Huang</a> , <em>Jiayun Wang</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Anima Anandkumar, Andrew J Hung' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Machine Learning for Health, PMLR</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v225/kocielnik23a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2312.03231" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper won the best paper award at 2023 Machine Learning for Health Conference.</p> </div> <div class="abstract hidden"> <p>Quantification of real-time informal feedback delivered by an experienced surgeon to a trainee during surgery is important for skill improvements in surgical training. Such feedback in the live operating room is inherently multimodal, consisting of verbal conversations (e.g., questions and answers) as well as non-verbal elements (e.g., through visual cues like pointing to anatomic elements). In this work, we leverage a clinically-validated five-category classification of surgical feedback: “Anatomic” , “Technical” , “Procedural” , “Praise” and “Visual Aid” . We then develop a multi-label machine learning model to classify these five categories of surgical feedback from inputs of text, audio, and video modalities. The ultimate goal of our work is to help automate the annotation of real-time contextual surgical feedback at scale. This work offers an important first look at the feasibility of automated classification of real-world live surgical feedback based on text, audio, and video modalities.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>TPAMI</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/oltr++_logo-480.webp 480w,/assets/img/publication_preview/oltr++_logo-800.webp 800w,/assets/img/publication_preview/oltr++_logo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/oltr++_logo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oltr++_logo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="liu2022open" class="col-sm-8"> <div class="title">Open Long-Tailed Recognition in a Dynamic World</div> <div class="author"> <a href="https://liuziwei7.github.io/" rel="external nofollow noopener" target="_blank">Ziwei Liu</a>, <a href="https://scholar.google.com/citations?user=at4m2mYAAAAJ" rel="external nofollow noopener" target="_blank">Zhongqi Miao</a>, <a href="https://xiaohangzhan.github.io/" rel="external nofollow noopener" target="_blank">Xiaohang Zhan</a> , <em>Jiayun Wang</em>, <a href="http://boqinggong.info/" rel="external nofollow noopener" target="_blank">Boqing Gong</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.ieeecomputersociety.org/10.1109/TPAMI.2022.3200091" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2208.08349.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Real world data often exhibits a long-tailed and open-ended (i.e., with unseen classes) distribution. A practical recognition system must balance between majority (head) and minority (tail) classes, generalize across the distribution, and acknowledge novelty upon the instances of unseen classes (open classes). We define Open Long-Tailed Recognition++ (OLTR++) as learning from such naturally distributed data and optimizing for the classification accuracy over a balanced test set which includes both known and open classes. OLTR++ handles imbalanced classification, few-shot learning, open-set recognition, and active learning in one integrated algorithm, whereas existing classification approaches often focus only on one or two aspects and deliver poorly over the entire spectrum. The key challenges are: 1) how to share visual knowledge between head and tail classes, 2) how to reduce confusion between tail and open classes, and 3) how to actively explore open classes with learned knowledge. Our algorithm, OLTR++, maps images to a feature space such that visual concepts can relate to each other through a memory association mechanism and a learned metric (dynamic meta-embedding) that both respects the closed world classification of seen classes and acknowledges the novelty of open classes. Additionally, we propose an active learning scheme based on visual memory, which learns to recognize open classes in a data-efficient manner for future expansions. On three large-scale open long-tailed datasets we curated from ImageNet (object-centric), Places (scene-centric), and MS1M (face-centric) data, as well as three standard benchmarks (CIFAR-10-LT, CIFAR-100-LT, and iNaturalist-18), our approach, as a unified framework, consistently demonstrates competitive performance. Notably, our approach also shows strong potential for the active exploration of open classes and the fairness analysis of minority groups.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>ECCV</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sketch2scene-480.webp 480w,/assets/img/publication_preview/sketch2scene-800.webp 800w,/assets/img/publication_preview/sketch2scene-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sketch2scene.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sketch2scene.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2022unsupervised" class="col-sm-8"> <div class="title">Unsupervised Scene Sketch to Photo Synthesis</div> <div class="author"> <em>Jiayun Wang</em>, Sangryul Jeon, <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>, Xi Zhang, <a href="https://www.linkedin.com/in/himanshu-arora-89b6365" rel="external nofollow noopener" target="_blank">Himanshu Arora</a>, and Yu Lou </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2209.02834.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://drive.google.com/file/d/1QaMZ0rj9L11DvR-zYKJSylAMP8gaIr29/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiQoZjVm4H6AhW1DkQIHdKFDfoQFnoECBEQAQ&amp;url=https%3A%2F%2Fwww.marktechpost.com%2F2022%2F09%2F06%2Fresearchers-from-uc-berkeley-and-amazon-introduce-an-unsupervised-ai-method-for-synthesizing-realistic-photos-from-scene-sketches%2F&amp;usg=AOvVaw1EMKqFBKNl7uGM_f1NsKtC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Unsupervised-Scene-Sketch-to-Photo-Synthesis" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://pwang.pw/scenesketch.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Sketches make an intuitive and powerful visual expression as they are fast executed freehand drawings. We present a method for synthesizing realistic photos from scene sketches. Without the need for sketch and photo pairs, our framework directly learns from readily available large-scale photo datasets in an unsupervised manner. To this end, we introduce a standardization module that provides pseudo sketch-photo pairs during training by converting photos and sketches to a standardized domain, i.e. the edge map. The reduced domain gap between sketch and photo also allows us to disentangle them into two components: holistic scene structures and low-level visual styles such as color and texture. Taking this advantage, we synthesize a photo-realistic image by combining the structure of a sketch and the visual style of a reference photo. Extensive experimental results on perceptual similarity metrics and human perceptual studies show the proposed method could generate realistic photos with high fidelity from scene sketches and outperform state-of-the-art photo synthesis baselines. We also demonstrate that our framework facilitates a controllable manipulation of photo synthesis by editing strokes of corresponding sketches, delivering more fine-grained details than previous approaches that rely on region-level editing.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Nature Portfolio</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/demo-480.webp 480w,/assets/img/publication_preview/demo-800.webp 800w,/assets/img/publication_preview/demo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/demo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="demo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2022predicting" class="col-sm-8"> <div class="title">Predicting Demographics from Meibography Using Deep Learning</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D Graham</a>, <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>, and <a href="https://eyecare.berkeley.edu/doctors/meng-lin/" rel="external nofollow noopener" target="_blank">Meng C Lin</a> </div> <div class="periodical"> <em>Nature Scientific reports</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.nature.com/articles/s41598-022-18933-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2022meiboNature.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.healio.com/news/optometry/20221104/ai-approach-predicts-demographic-clinical-features-from-meibography-images" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> </div> <div class="abstract hidden"> <p>This study introduces a deep learning approach to predicting demographic features from meibography images. A total of 689 meibography images with corresponding subject demographic data were used to develop a deep learning model for predicting gland morphology and demographics from images. The model achieved on average 77%, 76%, and 86% accuracies for predicting Meibomian gland morphological features, subject age, and ethnicity, respectively. The model was further analyzed to identify the most highly weighted gland morphological features used by the algorithm to predict demographic characteristics. The two most important gland morphological features for predicting age were the percent area of gland atrophy and the percentage of ghost glands. The two most important morphological features for predicting ethnicity were gland density and the percentage of ghost glands. The approach offers an alternative to traditional associative modeling to identify relationships between Meibomian gland morphological features and subject demographic characteristics. This deep learning methodology can currently predict demographic features from de-identified meibography images with better than 75% accuracy, a number which is highly likely to improve in future models using larger training datasets, which has significant implications for patient privacy in biomedical imaging.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">DIRA @ ECCV</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sketch-480.webp 480w,/assets/img/publication_preview/sketch-800.webp 800w,/assets/img/publication_preview/sketch-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sketch.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sketch.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang20223d" class="col-sm-8"> <div class="title">3D Shape Reconstruction from Free-Hand Sketches</div> <div class="author"> <em>Jiayun Wang</em> , <a href="https://jieruilin.github.io/" rel="external nofollow noopener" target="_blank">Jierui Lin</a> , <a href="https://jieruilin.github.io/" rel="external nofollow noopener" target="_blank">Qian Yu</a> , Runtao Liu, <a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In European Conference on Computer Vision</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Spotlight</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2006.09694" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2006.09694.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/samaonline/3D-Shape-Reconstruction-from-Free-Hand-Sketches" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper was selected as an spotlight presentation.</p> </div> <div class="abstract hidden"> <p></p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MI @ NeurIPS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/TFLL_tracking-480.webp 480w,/assets/img/publication_preview/TFLL_tracking-800.webp 800w,/assets/img/publication_preview/TFLL_tracking-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/TFLL_tracking.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="TFLL_tracking.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kothapalli2022tracking" class="col-sm-8"> <div class="title">Tracking the Dynamics of the Tear Film Lipid Layer</div> <div class="author"> <a href="https://tejasvikothapalli.github.io/" rel="external nofollow noopener" target="_blank">Tejasvi Kothapalli</a>, Charlie Shou, Jennifer Ding , <em>Jiayun Wang</em>, <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D Graham</a>, Tatyana Svitova, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Stella Yu, Meng C Lin' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In NeurIPS Workshop</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2212.03450" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202022/63531.png?t=1669932150.7934783" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a> <a href="https://docs.google.com/presentation/d/1S_VPrUFdYSddqJNQ4zFXWvZZghucO2bBMb7R-ai7FzA/edit?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a> <a href="https://easytear-dev.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Dry Eye Disease (DED) is one of the most common ocular diseases: over five percent of US adults suffer from DED [4]. Tear film instability is a known factor for DED, and is thought to be regulated in large part by the thin lipid layer that covers and stabilizes the tear film. In order to aid eye related disease diagnosis, this work proposes a novel paradigm in using computer vision techniques to numerically analyze the tear film lipid layer (TFLL) spread. Eleven videos of the tear film lipid layer spread are collected with a micro-interferometer and a subset are annotated. A tracking algorithm relying on various pillar computer vision techniques is developed.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>TPAMI</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spn-480.webp 480w,/assets/img/publication_preview/spn-800.webp 800w,/assets/img/publication_preview/spn-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/spn.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spn.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2021transformer" class="col-sm-8"> <div class="title">Spatial Transformer for 3D Point Clouds</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1906.10887.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/UAijTLXkupQ" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/samaonline/spatial-transformer-for-3d-point-clouds" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/spn_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="https://pwang.pw/spn.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Deep neural networks can efficiently process 3D point clouds. At each point convolution layer, local features can be learned from local neighborhoods of the point cloud. These features are combined together for further processing in order to extract the semantic information encoded in the point cloud. Previous networks adopt all the same local neighborhoods at different layers, as they utilize the same metric on fixed input point coordinates to define local neighborhoods. It is easy to implement but not necessarily optimal. Ideally local neighborhoods should be different at different layers so as to adapt to layer dynamics for efficient feature learning. One way to achieve this is to learn different transformations of the input point cloud at each layer, and then extract features from local neighborhoods defined on transformed coordinates. In this work, we propose a novel end-to-end approach to learn different non-rigid transformations of the input point cloud for different local neighborhoods at each layer. We propose both linear (affine) and non-linear (projective and deformable) spatial transformers for 3D point clouds.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">OVS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mgseg-480.webp 480w,/assets/img/publication_preview/mgseg-800.webp 800w,/assets/img/publication_preview/mgseg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/mgseg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mgseg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2021quantifying" class="col-sm-8"> <div class="title">Quantifying meibomian gland morphology using artificial intelligence</div> <div class="author"> <em>Jiayun Wang</em> , Shixuan Li , Thao N Yeh, <a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a>, <a href="https://www.linkedin.com/in/andrew-d-graham-58401240/" rel="external nofollow noopener" target="_blank">Andrew D Graham</a>, <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Meng C Lin' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Optometry and Vision Science</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://pubmed.ncbi.nlm.nih.gov/34469930/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://escholarship.org/content/qt7745h7zm/qt7745h7zm.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/samaonline/gland-segmentation-release" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Quantifying meibomian gland morphology from meibography images is used for the diagnosis, treatment, and management of meibomian gland dysfunction in clinics. A novel and automated method is described for quantifying meibomian gland morphology from meibography images. Meibomian gland morphological abnormality is a common clinical sign of meibomian gland dysfunction, yet there exist no automated methods that provide standard quantifications of morphological features for individual glands. This study introduces an automated artificial intelligence approach to segmenting individual meibomian gland regions in infrared meibography images and analyzing their morphological features.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>CVPR</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/orth-480.webp 480w,/assets/img/publication_preview/orth-800.webp 800w,/assets/img/publication_preview/orth-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/orth.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="orth.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2020orthogonal" class="col-sm-8"> <div class="title">Orthogonal Convolutional Neural Networks</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://yubeichen.com/" rel="external nofollow noopener" target="_blank">Yubei Chen</a>, <a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1911.12207.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/GVT8syBZXOo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://blog.csdn.net/MTandHJ/article/details/105716261" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/samaonline/Orthogonal-Convolutional-Neural-Networks" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/ocnn_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/ocnn_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> <a href="https://pwang.pw/ocnn.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Deep convolutional neural networks are hindered by training instability and feature redundancy towards further performance improvement. A promising solution is to impose orthogonality on convolutional filters. We develop an efficient approach to impose filter orthogonality on a convolutional layer based on the doubly block-Toeplitz matrix representation of the convolutional kernel instead of using the common kernel orthogonality approach, which we show is only necessary but not sufficient for ensuring orthogonal convolutions. Our proposed orthogonal convolution requires no additional parameters and little computational overhead. This method consistently outperforms the kernel orthogonality alternative on a wide range of tasks such as image classification and inpainting under supervised, semi-supervised and unsupervised settings. Further, it learns more diverse and expressive features with better training stability, robustness, and generalization. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>CVPR</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/longtail_logo-480.webp 480w,/assets/img/publication_preview/longtail_logo-800.webp 800w,/assets/img/publication_preview/longtail_logo-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/longtail_logo.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="longtail_logo.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Liu_2019_CVPR" class="col-sm-8"> <div class="title">Large-Scale Long-Tailed Recognition in an Open World</div> <div class="author"> <a href="https://liuziwei7.github.io/" rel="external nofollow noopener" target="_blank">Ziwei Liu</a>, <a href="https://scholar.google.com/citations?user=at4m2mYAAAAJ" rel="external nofollow noopener" target="_blank">Zhongqi Miao</a>, <a href="https://xiaohangzhan.github.io/" rel="external nofollow noopener" target="_blank">Xiaohang Zhan</a> , <em>Jiayun Wang</em>, <a href="http://boqinggong.info/" rel="external nofollow noopener" target="_blank">Boqing Gong</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Oral</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/1904.05160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://bair.berkeley.edu/blog/2019/05/13/oltr/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Blog</a> <a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://liuziwei7.github.io/projects/LongTail.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper was selected as an oral presentation (2.5%).</p> </div> <div class="abstract hidden"> <p>Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PBVS @ CVPR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/complex-480.webp 480w,/assets/img/publication_preview/complex-800.webp 800w,/assets/img/publication_preview/complex-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/complex.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="complex.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chakraborty_2019_CVPR_Workshops" class="col-sm-8"> <div class="title">Sur-Real: Frechet Mean and Distance Transform for Complex-Valued Deep Learning</div> <div class="author"> <a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a> , <em>Jiayun Wang</em>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/document/9025675" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1906.10048.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/sutkarsh/cds" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>The paper won the best paper award at 2019 CVPR workshop.</p> </div> <div class="abstract hidden"> <p>We develop a novel deep learning architecture for naturally complex valued data, which are often subject to complex scaling ambiguity. We treat each sample as a field in the space of complex numbers. With the polar form of a complex number, the general group that acts on this space is the product of planar rotation and non-zero scaling. This perspective allows us to develop not only a novel convoluation operator using weighted Fréchet mean (wFM) on a Riemannian manifold, but also to a novel fully connected layer operator using the distance to the wFM, with natural equivariant properties to non-zero scaling and planar rotations for the former and invariance properites for the latter. We demonstrate our method on widely used SAR dataset MSTAR and RadioML dataset.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#00369f"> <div>Nature Portfolio</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/feature_-480.webp 480w,/assets/img/publication_preview/feature_-800.webp 800w,/assets/img/publication_preview/feature_-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/feature_.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="feature_.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="miao2019insights" class="col-sm-8"> <div class="title">Insights and Approaches Using Deep Learning to Classify Wildlife</div> <div class="author"> <a href="https://scholar.google.com/citations?user=at4m2mYAAAAJ" rel="external nofollow noopener" target="_blank">Zhongqi Miao</a> , Kaitlyn M Gaynor , <em>Jiayun Wang</em>, <a href="https://liuziwei7.github.io/" rel="external nofollow noopener" target="_blank">Ziwei Liu</a>, Oliver Muellerklein, Mohammad Sadegh Norouzzadeh, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Alex McInturff, Rauri CK Bowie, Ran Nathan, Stella Yu, others' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Nature Scientific Reports</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.nature.com/articles/s41598-019-44565-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2019wildlifeSR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>The implementation of intelligent software to identify and classify objects and individuals in visual fields is a technology of growing importance to operatives in many fields, including wildlife conservation and management. This study applies advanced software to classify wildlife species using camera-trap data. We trained a convolutional neural network (CNN) to classify 20 African species with 87.5% accuracy from 111,467 images. Gradient-weighted class activation mapping (Grad-CAM) revealed key features, and mutual information methods identified neurons responding strongly to specific species, exposing dataset biases. Hierarchical clustering produced a visual similarity dendrogram, and we evaluated the model’s ability to recognize known and unfamiliar species from images outside the training set.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TVST</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/aaseg-480.webp 480w,/assets/img/publication_preview/aaseg-800.webp 800w,/assets/img/publication_preview/aaseg-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/aaseg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="aaseg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2019deep" class="col-sm-8"> <div class="title">A Deep Learning Approach for Meibomian Gland Atrophy Evaluation in Meibography Images</div> <div class="author"> <em>Jiayun Wang</em> , Thao N Yeh, <a href="https://rudra1988.github.io/" rel="external nofollow noopener" target="_blank">Rudrasis Chakraborty</a>, <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a>, and <a href="https://eyecare.berkeley.edu/doctors/meng-lin/" rel="external nofollow noopener" target="_blank">Meng C Lin</a> </div> <div class="periodical"> <em>Translational Vision Science &amp; Technology</em>, Jun 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1167/tvst.8.6.37" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2019meiboTVST.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/samaonline/mg-psp" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Purpose: To develop a deep learning approach to digitally segmenting meibomian gland atrophy area and computing percent atrophy in meibography images. Methods: A total of 706 meibography images with corresponding meiboscores were collected and annotated for each one with eyelid and atrophy regions. The dataset was then divided into the development and evaluation sets. The development set was used to train and tune the deep learning model, while the evaluation set was used to evaluate the performance of the model. Conclusions: The proposed deep learning approach can automatically segment the total eyelid and meibomian gland atrophy regions, as well as compute percent atrophy with high accuracy and consistency. This provides quantitative information of the gland atrophy severity based on meibography images.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">PR</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/apm-480.webp 480w,/assets/img/publication_preview/apm-800.webp 800w,/assets/img/publication_preview/apm-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/apm.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="apm.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2018deep" class="col-sm-8"> <div class="title">Deep Ranking Model by Large Adaptive Margin Learning for Person Re-Identification</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://scholar.google.com/citations?user=2Drvv44AAAAJ" rel="external nofollow noopener" target="_blank">Sanping Zhou</a> , Jinjun Wang, and Qiqi Hou </div> <div class="periodical"> <em>Pattern Recognition</em>, Jun 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://www.sciencedirect.com/science/article/pii/S0031320317303771" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1707.00409" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/samaonline/Deep-Ranking-Model-by-Large-Adaptive-Margin-Learning-for-Person-Re-identification" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Person re-identification aims to match images of the same person across disjoint camera views, which is a challenging problem in video surveillance. The major challenge of this task lies in how to preserve the similarity of the same person against large variations caused by complex backgrounds, mutual occlusions and different illuminations, while discriminating the different individuals. In this paper, we present a novel deep ranking model with feature learning and fusion by learning a large adaptive margin between the intra-class distance and inter-class distance to solve the person re-identification problem. Specifically, we organize the training images into a batch of pairwise samples. Treating these pairwise samples as inputs, we build a novel part-based deep convolutional neural network (CNN) to learn the layered feature representations by preserving a large adaptive margin. As a result, the final learned model can effectively find out the matched target to the anchor image among a number of candidates in the gallery image set by learning discriminative and stable feature representations. Overcoming the weaknesses of conventional fixed-margin loss functions, our adaptive margin loss function is more appropriate for the dynamic feature space. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <div>CVPR</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/p2s-480.webp 480w,/assets/img/publication_preview/p2s-800.webp 800w,/assets/img/publication_preview/p2s-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/p2s.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="p2s.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhou2017point" class="col-sm-8"> <div class="title">Point to Set Similarity Based Deep Feature Learning for Person Re-Identification</div> <div class="author"> <a href="https://scholar.google.com/citations?user=2Drvv44AAAAJ" rel="external nofollow noopener" target="_blank">Sanping Zhou</a> , Jinjun Wang , <em>Jiayun Wang</em> , Yihong Gong, and Nanning Zheng </div> <div class="periodical"> <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Point_to_Set_CVPR_2017_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Person re-identification (Re-ID) remains a challenging problem due to significant appearance changes caused by variations in view angle, background clutter, illumination condition and mutual occlusion. To address these issues, conventional methods usually focus on proposing robust feature representation or learning metric transformation based on pairwise similarity, using Fisher-type criterion. The recent development in deep learning based approaches address the two processes in a joint fashion and have achieved promising progress. One of the key issues for deep learning based person Re-ID is the selection of proper similarity comparison criteria, and the performance of learned features using existing criterion based on pairwise similarity is still limited, because only Point to Point (P2P) distances are mostly considered. In this paper, we present a novel person Re-ID method based on Point to Set similarity comparison. The Point to Set (P2S) metric can jointly minimize the intra-class distance and maximize the interclass distance, while back-propagating the gradient to optimize parameters of the deep model.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/sar-480.webp 480w,/assets/img/publication_preview/sar-800.webp 800w,/assets/img/publication_preview/sar-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/sar.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sar.gif" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2017successive" class="col-sm-8"> <div class="title">Successive Embedding and Classification loss for Aerial Image Classification</div> <div class="author"> <em>Jiayun Wang</em>, <a href="https://www.cs.cmu.edu/~pvirtue/" rel="external nofollow noopener" target="_blank">Patrick Virtue</a>, and <a href="https://web.eecs.umich.edu/~stellayu/" rel="external nofollow noopener" target="_blank">Stella Yu</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:1712.01511</em>, Jun 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/1712.01511" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/1712.01511.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/samaonline/AIC_dualloss" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Deep neural networks can be effective means to automatically classify aerial images but is easy to overfit to the training data. It is critical for trained neural networks to be robust to variations that exist between training and test environments. To address the overfitting problem in aerial image classification, we consider the neural network as successive transformations of an input image into embedded feature representations and ultimately into a semantic class label, and train neural networks to optimize image representations in the embedded space in addition to optimizing the final classification score. We demonstrate that networks trained with this dual embedding and classification loss outperform networks with classification loss only. We also find that moving the embedding loss from commonly-used feature space to the classifier space, which is the space just before softmax nonlinearization, leads to the best classification performance for aerial images. Visualizations of the network’s embedded representations reveal that the embedding loss encourages greater separation between target class clusters for both training and testing partitions of two aerial image classification benchmark datasets.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Jiayun (Peter) Wang. Last updated: November 23, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>